\documentclass[fleqn]{article}\usepackage[]{graphicx}\usepackage[]{color}

\usepackage{geometry} 
\geometry{letterpaper, top=1.5cm, left=2cm, right=2cm}                

\usepackage[affil-it]{authblk}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{booktabs}
\usepackage{array}
%\usepackage{paralist}
\usepackage{natbib}
\usepackage{enumerate}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{setspace}

\include{GrandMacros}
\newcommand{\norm}[1]{\vert\vert #1 \vert\vert}
\newcommand{\ind}{\mathbb{I}}

\onehalfspacing

\title{The Method of Analogues}
\date{}
\author{Evan L. Ray\thanks{Email: \texttt{elray@umass.edu}; Corresponding author}, Krzysztof Sakrejda, Stephen A. Lauer, Alexandria C. Brown, Xi Meng, and Nicholas G. Reich}
\affil{Department of Biostatistics and Epidemiology, \\ University of Massachusetts -- Amherst, Amherst, MA}

\begin{document}

<<InitialBlock, echo = FALSE>>=
opts_knit$set(concordance=TRUE)
opts_chunk$set(echo=FALSE)

suppressMessages(library(lubridate))

suppressMessages(library(grid))
suppressMessages(library(ggplot2))

suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape))

suppressMessages(library(ssr))
@

\maketitle

\section{Introduction}
\label{sec:Intro}

In this document, we describe the method we use for predicting Dengue fever incidence in the 2015 Dengue Forecasting Project.  Briefly, we employ a non-parametric approach to prediction in dynamical systems referred to as state space reconstruction or the method of analogues (MoA).  Our implementation of MoA is similar in spirit to the simplex projection method of \cite{sugihara1990nonlinearForecasting}; however, we frame the approach in terms of kernel regression and kernel density estimation \citep{silvermanDE, racine2004nonparametricregression}.

The remainder of this document is organized as follows.  We describe the general motivation for MoA in Section \ref{sec:Overview}, give some details for our implementation based on kernel methods in Section \ref{sec:MethodDescription}, and discuss our strategy for estimating the model parameters in Section \ref{sec:Estimation}.

\section{Overview of the Method of Analogues}
\label{sec:Overview}

The spread of an infectious disease within a population is often represented as a dynamical process.  For example, the SIR model and its variants partition the population into disjoint groups of people according to their infection status, and describe changes in the size of each group over time with a system of differential equations.  These models make several assumptions regarding the number of states (i.e., disease categories) and the functional forms describing the rates at which people move through these categories over time.

A central challenge in predicting infectious disease outbreaks is that we typically observe many fewer covariates than are important to the evolution of the process, and the functional forms describing how these covariates enter into the model are not known precisely.  For example, even the most basic formulation of the SIR model involves the number of Susceptible and Infected individuals in the population at each time point.  Most infectious disease data sets include observations of the number of infected individuals in the population, but do not include measures of the number of susceptible individuals.  Furthermore, the SIR model is a very simplified representation of the evolution of the disease.  In reality, there are likely to be many other important factors affecting disease dynamics such as weather, immigration, and the presence of other diseases.  Many of these other factors may also be unobserved.

MoA is an alternate, non-parametric aproach to learning about dynamical systems that can be applied when we are uncertain of the functional forms describing evolution of the process and we do not have observations of all of the state variables.  The method can be motivated by Takens' theorem \citep{takens1981detectingattractors}, which establishes a connection between the original dynamical process and the process that is obtained by forming vectors of lagged observations.  Roughly, the theorem states that if the dynamical process satisfies certain conditions, there is a smooth, one-to-one and onto mapping with a smooth inverse between the limit sets of the original dynamical process and the limit sets of the lagged observation process. This provides some justification for studying the lagged observation process in order to learn about long-term dynamics of the system.  The theorem also tells us that we must include at least $2D + 1$ lags in the lagged observations in order to guarantee that the limit sets are equivalent in the sense described above, where $D$ is the dimension of the state space of the underlying dynamical process.

Although this theorem can help motivate the study of the lagged observation process, it does not answer two key questions for a practical implementation:
\begin{enumerate}
  \item If the dimension $D$ of the underlying process is unknown, we do not know how many or which lags should be included when we form the lagged observation process.
  \item It does not provide us with a specific method for studying the lagged observation process or using it to perform prediction.
\end{enumerate}

\lboxit{To do: Insert some pictures and a brief description of the intuitive idea.}

We use a non-parametric approach based on kernel regression (for point predictions) and kernel density estimation (for predictive distributions) to learn the relationship between the lagged observation process and its future values.  We use cross-validation to select the lags that are included and estimate the bandwidth parameters for the kernel functions.  We give a more formal statement of the method and our estimation procedures in Sections \ref{sec:MethodDescription} and \ref{sec:Estimation}.

%\begin{figure}[t!]
%\centering
%\begin{subfigure}[t]{0.475\textwidth}
%<<SJPRWeeklyCaseCounts, fig.height=3.5, fig.width=3.5>>=
%sj <- San_Juan_train
%
%p <- ggplot(sj[sj$season %in% c("1990/1991", "1991/1992", "1992/1993"), ],
%  aes(x=week_start_date, y=total_cases)) +
%  geom_line() +
%  xlab("Time") +
%  ylab("Total Weekly Cases") +
%  ggtitle("\nTotal Weekly Cases") +
%  theme_bw()
%print(p)
%@
%\caption{Total Weekly Cases over time}
%\end{subfigure}
%~
%\begin{subfigure}[t]{0.475\textwidth}
%<<SJPRWeeklyCaseCounts2, fig.height=3.5, fig.width=3.5>>=
%sj <- sj %>% mutate(total_cases_lag1 = lag(total_cases))
%
%p <- ggplot(sj[sj$season %in% c("1990/1991", "1991/1992", "1992/1993") & !is.na(sj$total_cases_lag1), ],
%  aes(x=total_cases_lag1, y=total_cases, colour=season)) +
%  geom_path() +
%  xlab("Lagged Total Weekly Cases") +
%  ylab("Total Weekly Cases") +
%  ggtitle("Lagged Total Weekly Cases\nvs Total Weekly Cases") +
%  theme_bw()
%print(p)
%@
%\caption{Lagged Total Weekly Cases vs Total Weekly Cases}
%\end{subfigure}
%\caption{Total cases by week for San Juan, Puerto Rico.}
%\label{fig:SJPRWeeklyCaseCounts}
%\end{figure}

\section{Method Description}
\label{sec:MethodDescription}

Suppose we observe $\bz_t = \{z_{t,1}, \ldots, z_{t,D}\} \in \mathbb{R}^D$ at each point in time $t = 1, \ldots, T$.  Our goal is to obtain a predictive distribution for one of the observed variables, with index $d_{pred} \in \{1, \ldots, D\}$, over a range of prediction horizons contained in the set $\mathcal{P}$.  For example, if we have weekly data and we are interested in obtaining predictions for a range between 4 and 6 weeks after the most recent observation then $\mathcal{P} = \{4, 5, 6\}$.  Let $P$ be the largest element of the set $\mathcal{P}$ of prediction horizons.

In order to perform prediction, we will use lagged observations.  Let $\bl^{max} = (l^{max}_1, \ldots, l^{max}_D)$ specify the maximum number of lags for each observed variable that may be used for prediction, and let $L = \max{d} l^{max}_d$ be the overall largest lag that may be used across all variables.  In the estimation procedure we describe in Section~\ref{sec:Estimation}, we will select a subset of these lags to actually use in the predictions.  We capture which lags are actually used in the vector 
\begin{align*}
&\bu = (u_{1,0}, \ldots, u_{1, l^{max}_1}, \ldots, u_{D,0}, \ldots, u_{D, l^{max}_D}) \text{, where} \\
&u_{d, l} = \begin{cases} 0 \text{ if lag $l$ of variable $d$ is not used in forming predictions} \\ 1 \text{ if lag $l$ of variable $d$ is used in forming predictions.} \end{cases}
\end{align*}

By analogy with the standard notation in autoregressive models, we define
\begin{align*}
&\by_t = (z_{t, d_{pred}}, \ldots, B^{(P - 1)} z_{t, d_{pred}}) \text{ and} \\
&\bx_t = (B^{(P)} z_{t, 1}, \ldots, B^{(P + l^{max}_1 - 1)} z_{t, 1}, \ldots, B^{(P)} z_{t, D}, \ldots, B^{(P + l^{max}_D - 1)} z_{t, D})
\end{align*}
Here, $B^{(a)}$ is the backshift operator defined by $B^{(a)} z_{t, d} = z_{t - a, d}$.  Note that the lengths of $\by_t$ and $\bx_t$, as well as exactly which lags are used to form them, depend on $\mathcal{P}$ and $\bl^{max}$; we suppress this dependence in the notation for the sake of clarity.  The vector $\by_t$ represents the prediction target when our most recent observation was made at time $t - P$: the vector of observed values at each prediction horizon $p \in \mathcal{P}$.  The variable $\bx_t$ represents the vector of all lagged covariates that are available for use in performing prediction.

To make the notation concrete, suppose that $\bz_t$ contains the observed case count for week $t$ in San Juan, the observed case count for week $t$ in Iquitos, and the date on Monday of week $t$, and our goal is to predict the weekly case count in San Juan.  Then $D = 3$ and $d_{pred} = 1$.  If we want to predict the weekly case counts for the two weeks after the most recently observation, then $p = 2$.  If we specify that the model may include the two most recent observations for the case counts in San Juan and Iquitos, but only the time index at the most recent observation then $\bl^{max} = (1, 1, 0)$.  If our current model uses only the most recently observed case counts for San Juan and Iquitos then $\bu = (1, 0, 1, 0, 0)$, where the 1's are in the positions of the $\bu$ vector representing lag 0 of the counts for San Juan and lag 0 of the counts for Iquitos.  The variable $y_t^{(P)}$ is a vector containing the observed case counts for San Juan in weeks $t + 1$ and $t + 2$; $\bx_t^{(\bl^{max})}$ contains the observed case counts for San Juan and Iquitos in weeks $t$ and $t - 1$ as well as the time index variable in week $t$.

In order to perform prediction, we regard $\{(\by_t, \bx_t), t = 1 + P + L, \ldots, T\}$ as a sample from the joint distribution of $(\bY, \bX)$.  We wish to estimate the conditional distribution of $\bY | \bX$.  In order to do this, we employ kernel density estimation.  Let $K^{\bY}(\by, \by^*, H^{\bY})$ and $K^{\bX}(\bx, \bx^*, H^{\bX})$ be kernel functions centered at $\by^*$ and $\bx^*$ respectively and with bandwidth matrices $H^{\bY}$ and $H^{\bX}$.  We estimate the conditional distribution of $\bY | \bX$ as follows:
\begin{align}
&\widehat{f}_{\bY|\bX}(\by | \bX = \bx) = \frac{\widehat{f}_{\bY, \bX}(\by, \bx)}{\widehat{f}_{\bX}(\bx)} \label{eqn:KDECondDef} \\
&\qquad = \frac{\sum_{t \in \tau} K^{\bY}(\by, \by_t, H^{\bY}) K^{\bX}(\bx, \bx_t, H^{\bX})}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
&\qquad = \sum_{t \in \tau} w_t K^{\bY}(\by, \by_t, H^{\bY}) \text{, where} \label{eqn:KDEwt} \\
&w_t = \frac{ K^{\bX}(\bx, \bx_t, H^{\bX}) }{\sum_{t^* \in \tau} K^{\bX}(\bx, \bx_{t^*}, H^{\bX}) } \label{eqn:KDEWeightsDef}
\end{align}

In Equation~\eqref{eqn:KDECondDef}, we are simply making use of the fact that the conditional density for $\bY | \bX$ can be written as the quotient of the joint density for $(\bY, \bX)$ and the marginal density for $\bX$.  In Equation~\eqref{eqn:KDESubKDEJtMarginal}, we obtain separate kernel density estimates for the joint and marginal densities in this quotient.  In Equation~\eqref{eqn:KDEwt}, we rewrite this quotient by passing the denominator of Equation~\eqref{eqn:KDESubKDEJtMarginal} into the summation in the numerator.  We can interpret the result as a weighted kernel density estimate, where each observation $t \in \tau$ contributes a different amount to the final conditional density estimate.  The amount of the contribution from observation $t$ is given by the weight $w_t$, which effectively measures how similar $\bx_t$ is to the point $\bx$ at which we are estimating the conditional density.  If $\bx_t^{(\bl^{max})}$ is similar to $\bx_{t^*}^{(\bl^{max})}$, a large weight is assigned to $t$; if $\bx_t^{(\bl^{max})}$ is different from $\bx_{t^*}^{(\bl^{max})}$, a small weight is assigned to $t$.

In kernel density estimation, it is generally required that the kernel functions integrate to $1$ in order to obtain valid density estimates.  However, after conditioning on $\bX$, it is no longer necessary that $K^{\bX}(\bx, \bx_t, H^{\bX})$ integrate to $1$.  In fact, as can be seen from Equation~\eqref{eqn:KDEWeightsDef}, any multiplicative constants of proportionality will cancel out when we form the observation weights.  We can therefore regard $K^{\bX}(\bx, \bx_t, H^{\bX})$ as a more general weighting function that measures the similarity between $\bx$ and $\bx_t$.  As we will see, eliminating the constraint that $K^{\bX}$ integrates to $1$ is a useful expansion the space of functions that can be used in calculating the observation weights.  However, we still require that $K^{\bY}$ integrates to $1$.

In Equations \eqref{eqn:KDECondDef} through \eqref{eqn:KDEWeightsDef}, $\tau$ is an index set of time points used in obtaining the density estimate.  In most settings, we can take $\tau = \{1 + P + L, \ldots, T\}$.  These are the time points for which we can form the lagged observation vector $\bx_t$ and the prediction target vector $\by_t$.  However, we will place additional restrictions on the time points included in $\tau$ in the cross-validation procedure discussed in Section \ref{sec:Estimation}.

If we wish to obtain point predictions, we can use a summary of the predictive density.  For example, if we take the expected value, we obtain kernel regression:
\begin{align}
&(\widehat{\bY} | \bX = \bx) = \mathbb{E}_{\widehat{f}_{\bY|\bX}}\{\bY | \bX = \bx\} \label{eqn:PtPredDef} \\
&\qquad = \int \sum_{t \in \tau} w_t K^{\bY}(\by, \by_t, H^{\bY}) \by \, d \by  \label{eqn:PtPredKDE} \\
&\qquad = \sum_{t \in \tau} w_t \by_t  \label{eqn:PtPredFinal}
\end{align}
The equality in Equation~\eqref{eqn:PtPredFinal} holds if the kernel function $K^{\bY}(\by, \by_t, H^{\bY})$ is symmetric about $\by_t$, or more generally if it is the pdf of a random variable with expected value $\by_t$.

In our work, we have used product kernel functions for $K^{\bY}(\by, \by_t, H^{\bY})$ and $K^{\bX}(\bx, \bx_t, H^{\bX})$:
\begin{align*}
&K^{\bY}(\by, \by_t, H^{\bY}) = \prod_{p \in \mathcal{P}} K^{\bY}_p(\by_p, \by_{t, p}], h^{\bY}_p) \\
&K^{\bX}(\bx, \bx_t, H^{\bX}) = \prod_{v = 1}^V K^{\bX}_{v}(\bx_{v}, \bx_{t, v}, H^{\bX}_{v}) \text{, where $V = \sum_{d = 1}^D \bl^{max}_d$ is the total length of $\bX$.}
\end{align*}
The use of product kernels may not be optimal; this aspect of our approach could be revised in future work.

We have employed two functional forms for the component univariate kernel functions in our work.  We use the squared exponential kernel for most of the component kernel functions in both $K^{\bY}$ and $K^{\bX}$.  This function has the following form:
\begin{align*}
&K(x, x^*, h) \varpropto \exp\left\{ -\frac{1}{2} \left(\frac{x - x^*}{h}\right)^2 \right\}
\end{align*}
The constant of proportionality can be ignored when this is used as a component kernel function in $K^{\bX}$; when used as a component function in $K^{\bY}$, is is resolved by the constraint that the kernel function integrate to $1$.  It should be noted that the use of this kernel function in $K^{\bY}$ is technically inappropriatefor our application, since the prediction target (case counts) is a discrete random variable but the squared exponential kernel is defined for continuous values.  We have adopted the squared exponential kernel for this purpose due to time constraints, and we plan to pursue the use of discrete kernels in future work.

The second kernel we have employed in the weighting kernel $K^{wt}$ is the periodic kernel, which is commonly used in the literature on Gaussian Processes \citep{rasmussen2006gaussianprocesses}.  This kernel is defined as follows:
\begin{align*}
&K(x, x^*, h) \varpropto \exp\left[ -\frac{1}{2} sin^2\left\{\rho (x - x^*) \right\} / h^2 \right]
\end{align*}
The parameter $\rho$ determines the period of the kernel function.  In our work, we have fixed $\rho$ so that the kernel has a period of 1 year and estimated only the bandwidth $h$.  We have employed the periodic kernel only for the component of the weighting kernel corresponding to the time index variable.  Effectively, this means that observations are scored as being similar to each other if they were recorded at a similar time of the year.

There are several details specific to our application of the SSR method to the Dengue challenge data which we now discuss.  First, it can be seen from the plots in Figure \ref{fig:SJPRWeeklyCaseCounts} that the weekly case counts are not smooth over time.  The plot in panel (a) of the figure shows that the weekly case counts are jagged, with many small peaks around a larger trend.  In panel (b), we see that this induces some "loops" in the lagged covariate space when a lag of one week is used.  These loops can cause difficulties for the SSR method, because lagged covariate vectors representing different points in the disease's evolution may be near each other.  This problem can be addressed in several ways.  One possibility is the use of a different lag; for example, these loops might be reduced if a lag of three weeks were used instead of one week.

Another alternative that we pursue is the use of smoothed observations in forming the lagged observation vectors.  We use smoothed case counts on a log scale for the weighting kernels, and the unsmoothed case counts on the original scale for the prediction kernels.

\lboxit{To do:
\begin{itemize}
\item more detail about smoothing.
\item edge effects -- for each $x_t$, smooth up to time $t$
\item probably also smooth $y_t$
\item what smoothing method to use?
\end{itemize}
}

\section{Parameter Estimation}
\label{sec:Estimation}

We use cross-validation to select the variables that are used in the model and estimate the corresponding bandwidth parameters by (approximately) minimizing a cross-validation measure of the quality of the predictions obtained from the model.  Formally,
\begin{align}
&(\widehat{\bu}, \widehat{H}^{\bX}, \widehat{H}^{\bY}) \approx \argmin{(\bu, H^{\bX}, H^{\bY})} \sum_{t^* = 1 + P + L}^T Q[ \by_{t^*}, \widehat{f}(\by | \bX = \bx_{t^*} ; \bu, H^{\bX}, H^{\bY}, \{ (\by_t, \bx_t): t \in \tau_{t^*} \}) ] \label{eqn:ParamEst}
\end{align}
Here, $Q$ is a loss function that measures the quality of the estimated density $\widehat{f}$ given an observation $\by_{t^*}$.  We have made the dependence of this estimated density on the the parameters $\bu$, $H^{\bx}$, and $H^{\bY}$, as well as on the data $\{ (\by_t, \bx_t): t \in \tau_{t^*} \}$, explicit in the notation.  In order to reduce the potential for our parameter estimates to be affected by local correlation in the time series, we eliminate all time points that fall within one year of $t^*$ from the index set $\tau_{t^*}$ used to form the conditional density estimate $\widehat{f}(\by | \bX = \bx_{t^*} ; \bu, H^{\bX}, H^{\bY}, \{ (\by_t, \bx_t): t \in \tau_{t^*} \})$.

\lboxit{Talk about proper scoring rules and our particular choice of $Q$.}

We use a forward/backward stagewise procedure to obtain the set of combinations of variables and lags that are included in the final model (represented by $\bu$).  For each candidate model, we use the limited memory box constrained optimization procedure of \cite{byrd1995limitedmemoryoptim} to estimate the bandwidth parameters.  The approximation in Equation~\eqref{eqn:ParamEst} is due to the fact that this optimization procedure may not find a global minimum.

\section{Predictions for Challenge}
\label{sec:PredictionsForChallenge}



\section{Variables}
\label{sec:Variables}



\section{Computational Resources}
\label{sec:ComputationalResources}

We implemented our variation on SSR in the \texttt{R} statistical programming language \citep{RCore}.

\section{Publications}
\label{sec:Publications}

\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{plainnat}
\bibliography{SSRbib}
\endgroup

\end{document}
