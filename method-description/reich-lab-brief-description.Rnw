\documentclass[fleqn]{article}\usepackage[]{graphicx}\usepackage[]{color}

\usepackage{geometry} 
\geometry{letterpaper, top=1.5cm, left=2cm, right=2cm}                

\usepackage[affil-it]{authblk}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
\usepackage{booktabs}
\usepackage{array}
%\usepackage{paralist}
\usepackage{natbib}
\usepackage{enumerate}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\include{GrandMacros}
\newcommand{\norm}[1]{\vert\vert #1 \vert\vert}
\newcommand{\ind}{\mathbb{I}}

\usepackage{setspace}
\onehalfspacing


\title{Methods Description for Dengue Prediction \\ Reich Lab Team}
\date{}
\author{Evan L. Ray\thanks{Email: \texttt{elray@umass.edu}; Corresponding author}, Krzysztof Sakrejda, Stephen A. Lauer, Xi Meng, Nicholas G. Reich}
\affil{Department of Biostatistics and Epidemiology, \\ University of Massachusetts -- Amherst, Amherst, MA}

\begin{document}

<<InitialBlock, echo = FALSE>>=
opts_knit$set(concordance=TRUE)
opts_chunk$set(echo=FALSE)

suppressMessages(library(lubridate))

suppressMessages(library(grid))
suppressMessages(library(ggplot2))

suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape))

suppressMessages(library(ssr))
@

\maketitle

\section{Agreement}

By submitting these forecasts, we indicate our full and unconditional agreement to abide by the project's official rules and data use agreements.

\section{Introduction}
\label{sec:Intro}

In this document, we describe the method we use for predicting Dengue fever incidence in the 2015 Dengue Forecasting Project.  Briefly, we employ a non-parametric approach to prediction in dynamical systems referred to as state space reconstruction (SSR).  Our implementation of SSR is similar in spirit to the simplex projection method of \cite{sugihara1990nonlinearForecasting}; however, we frame the approach in terms of kernel regression and kernel density estimation \citep{silvermanDE, racine2004nonparametricregression}.

The remainder of this document is organized as follows.  We describe the general motivation for the SSR approach in Section \ref{sec:Overview}, give some details for our implementation of SSR based on kernel methods in Section \ref{sec:ModelDescription}, and discuss our strategy for estimating the model parameters in Section \ref{sec:Estimation}.  We then describe how we post-process the predictive distributions obtained through the SSR method in order to obtain predictions for the quantities required for the competition in Section \ref{sec:PostProcessing}.  We describe some details of our implementation of the method in the context of the competition, including the variables we used and that were selected in the fitting process in Section \ref{sec:Variables}.  Finally, we describe the computational resources that we used in Section \ref{sec:ComputationalResources} and list some references in Section \ref{sec:Publications}.

\section{Overview of State Space Reconstruction}
\label{sec:Overview}

The spread of an infectious disease within a population is often represented as a dynamical process.  For example, the SIR model and its variants partition the population into disjoint groups of people according to their infection status, and describe changes in the size of each group over time with a system of differential equations.  These models make several assumptions regarding the number of states (i.e., disease categories) and the functional forms describing the rates at which people move through these categories over time.

A central challenge in predicting infectious disease outbreaks is that we typically observe many fewer covariates than are important to the evolution of the process, and the functional forms describing how these covariates enter into the model are not known precisely.  For example, even the most basic formulation of the SIR model involves the number of Susceptible and Infected individuals in the population at each time point.  Most infectious disease data sets include observations of the number of infected individuals in the population, but do not include measures of the number of susceptible individuals.  Furthermore, the SIR model is a very simplified representation of the evolution of the disease.  In reality, there are likely to be many other important factors affecting disease dynamics such as weather, immigration, and the presence of other diseases.  Many of these other factors may also be unobserved.

SSR is an alternate, non-parametric aproach to learning about dynamical systems that can be applied when we are uncertain of the functional forms describing evolution of the process and we do not have observations of all of the state variables.  The method can be motivated by Takens' theorem \citep{takens1981detectingattractors}, which establishes a connection between the original dynamical process and the process that is obtained by forming vectors of lagged observations.  Roughly, the theorem states that if the dynamical process satisfies certain conditions, there is a smooth, one-to-one and onto mapping with a smooth inverse between the limit sets of the original dynamical process and the limit sets of the lagged observation process. This provides some justification for studying the lagged observation process in order to learn about long-term dynamics of the system.  The theorem also tells us that we must include at least $2D + 1$ lags in the lagged observations in order for the limit sets to be equivalent in the sense described above, where $D$ is the number of states in the state space of the underlying dynamical process.

Although this theorem can help motivate the study of the lagged observation process, it does not answer two key questions for a practical implementation:
\begin{enumerate}
  \item If the dimension $D$ of the underlying process is unknown, we do not know how many or which lags should be included when we form the lagged observation process.
  \item It does not provide us with a specific method for studying the lagged observation process or using it to perform prediction.
\end{enumerate}

For this challenge, we use a non-parametric approach based on kernel regression and kernel density estimation to learn the relationship between the lagged observation process and its future values.  We use cross-validation to select the lags that are included and estimate the bandwidth parameters for the kernel functions.  We give a more formal statement of the method and our estimation procedures in Sections \ref{sec:ModelDescription} and \ref{sec:Estimation}.

%, we give an intuitive overview of it using the Dengue data from the challenge.  For the purposes of this demonstration of the method, we use a lag of one in forming the lagged observation vector.  We plot the weekly case counts in Figure~\ref{fig:SJPRWeeklyCaseCounts:a} and their transformation into a lagged coordinate space in Figure~\ref{fig:SJPRWeeklyCaseCounts:b}.
%' 
%' \begin{figure}[t!]
%' \centering
%' \begin{subfigure}[t]{0.475\textwidth}
%' <<SJPRWeeklyCaseCounts, fig.height=3.5, fig.width=3.5>>=
%' sj <- San_Juan_train
%' 
%' p <- ggplot(sj[sj$season %in% c("1990/1991", "1991/1992", "1992/1993"), ],
%'   aes(x=week_start_date, y=total_cases)) +
%'   geom_line() +
%'   xlab("Time") +
%'   ylab("Total Weekly Cases") +
%'   ggtitle("\nTotal Weekly Cases") +
%'   theme_bw()
%' print(p)
%' @
%' \caption{Total Weekly Cases over time}
%' \end{subfigure}
%' ~
%' \begin{subfigure}[t]{0.475\textwidth}
%' <<SJPRWeeklyCaseCounts2, fig.height=3.5, fig.width=3.5>>=
%' sj <- sj %>% mutate(total_cases_lag1 = lag(total_cases))
%' 
%' p <- ggplot(sj[sj$season %in% c("1990/1991", "1991/1992", "1992/1993") & !is.na(sj$total_cases_lag1), ],
%'   aes(x=total_cases_lag1, y=total_cases, colour=season)) +
%'   geom_path() +
%'   xlab("Lagged Total Weekly Cases") +
%'   ylab("Total Weekly Cases") +
%'   ggtitle("Lagged Total Weekly Cases\nvs Total Weekly Cases") +
%'   theme_bw()
%' print(p)
%' @
%' \caption{Lagged Total Weekly Cases vs Total Weekly Cases}
%' \end{subfigure}
%' \caption{Total cases by week for San Juan, Puerto Rico.}
%' \label{fig:SJPRWeeklyCaseCounts}
%' \end{figure}

\section{Model Description}
\label{sec:ModelDescription}

Suppose we observe $\bz_t = \{z_{t,1}, \ldots, z_{t,D}\} \in \mathbb{R}^D$ at each point in time $t = 1, \ldots, T$.  For the purpose of the competition, our goal is to obtain a predictive distribution for one of the observed variables over a range of prediction horizons spanning the remainder of the current season using lagged observations of all of the variables.  In order to do this, we employ weighted kernel density estimation. We give a brief description of our methods here; a more complete description is available online \citep{Ray2015}.

We denote the index of the variable in $\bz_t$ that we are predicting by $d_{pred} \in \{1, \ldots, D\}$, and the prediction horizon by $P$.  Let $\bl^{max} = (l^{max}_1, \ldots, l^{max}_D)$ specify the maximum number of lags for each observed variable that may be used for prediction.  The lags that are actually used in prediction will be estimated by a procedure we describe in Section \ref{sec:Estimation}.  We define
\begin{align*}
&\by_t^{(P)} = (F^{(1)} z_{t, d_{pred}}, \ldots, F^{(P)} z_{t, d_{pred}}) \text{ and} \\
&\bx_t^{(\bl^{max})} = (z_{t, 1}, \ldots, B^{(l^{max}_1)} z_{t, 1}, \ldots, z_{t, D}, \ldots, B^{(l^{max}_D)} z_{t, D})
\end{align*}
where, $F^{(a)}$ is the forward shift operator defined by $F^{(a)} z_{t, d} = z_{t + a, d}$ and $B^{(a)}$ is the backward shift operator defined by $B^{(a)} z_{t, d} = z_{t - a, d}$.  The variable $\by_t^{(P)}$ represents the prediction target when our most recent observation was made at time $t$: the vector of counts for the following $P$ weeks.  The variable $\bx_t^{(\bl^{max})}$ represents the vector of all covariates and lagged covariates that are available for use in performing prediction.  We wish to estimate the distribution of $\by_t^{(P)} | \bx_t^{(\bl^{max})}$.

The general method for predicting $\by_{t^*}^{(P)}$ at some particular time $t^*$ can be summarized as follows. We use a weighting kernel to assign a weight $w_{t, t^*}$ to each time $t$ that reflects how similar the lagged observation vector $\bx_t^{(\bl^{max})}$ formed at time $t$ is to the lagged observation vector $\bx_{t^*}^{(\bl^{max})}$ formed at the time $t^*$.  If $\bx_t^{(\bl^{max})}$ is similar to $\bx_{t^*}^{(\bl^{max})}$, a large weight is assigned to $t$; if $\bx_t^{(\bl^{max})}$ is different from $\bx_{t^*}^{(\bl^{max})}$, a small weight is assigned to $t$.  These weights are then used to obtain a weighted kernel density estimate for $\by_{t^*}^{(P)}$ by placing a prediction kernel centered at $\by_t^{(P)}$ with the corresponding weight $w_{t, t^*}$.

In our work, both the weighting kernel and the prediction kernel have the form of product kernel functions, with one univariate component kernel function for each element in the $\bx_t^{(\bl^{max})}$ vector (for the weighting kernel) or the $\by_t^{(P)}$ vector (for the prediction kernel).  This choice is likely not optimal, and could be revised in future work.

In this work, we have employed two functional forms for the component univariate kernel functions.  We use the squared exponential kernel for most of the component kernel functions in both the weighting and prediction kernels.  When this kernel is used as a component in $K^{pred}$, we require that the kernel function integrates to 1 in order to obtain a proper density estimate.  The second functional form we have employed in the weighting kernel $K^{wt}$ is the periodic kernel, which is commonly used in the literature on Gaussian Processes \citep{rasmussen2006gaussianprocesses}.  We have employed the periodic kernel only for the component of the weighting kernel corresponding to the time index variable.  Effectively, this means that observations are scored as being similar to each other if they were recorded at a similar time of the year.

\section{Parameter Estimation}
\label{sec:Estimation}

We use cross-validation in order to select the variables that are used in the model and estimate the corresponding bandwidth parameters by (approximately) minimizing a cross-validation estimate of the mean absolute error (MAE) of the point predictions obtained from the model.  In order to estimate the absolute error associated with the point predictions $\widehat{\by}_{t^*}^{(P)}$ made at a particular time $t^*$, we discard all observations within 1 year of $t^*$ when calculating the weights $w_{t, t^*}$.  This reduces the potential for our parameter estimates to be affected by the presence of local information that would not be available when using the model fit for real-time prediction.

We use a forward/backward stagewise procedure to obtain the set of combinations of variables and lags that are included in the final model.  For each candidate model, we use the limited memory box constrained optimization procedure of \cite{byrd1995limitedmemoryoptim} to estimate the bandwidth parameters by (approximately) minimizing the MAE.

%In order to obtain the point estimate $\widehat{y}_{t^*}^{(p)}$ for each time point $t^*$ and number of steps ahead $p$, we first form the predictive density as in Equations \eqref{eqn:PredLine1} through \eqref{eqn:PredLine4}.  In order to reduce the potential for our parameter estimates to be affected by local correlation in the time series, we eliminate all time points that fall within one year of $t^*$ from the index set $\tau$; we refer to this timepoint-specific index set as $\tau^*$.  We then take our point estimate to be the expected value of this estimated predictive distribution. We note that for symmetric prediction kernels, this point estimate does not depend on the bandwidth parameters.

For the prediction kernels, we estimated the bandwidth parameters using the methods of \cite{sheather1991KDEbw}, as implemented in \texttt{R}'s \texttt{bw.SJ} function.  We estimate the bandwidth parameter for each prediction horizon $p \in \{1, \ldots, P\}$ separately.

\section{Post-Processing to Obtain Predictions for Challenge}
\label{sec:PostProcessing}

For the purposes of the challenge, we obtain point and distribution predictions by post-processing $N_{sims} = 10^5$ simulated case count trajectories drawn from the estimated density $\widehat{f}(\by_{t^*}^{(P)})$.  We complete these case count trajectories for the season by filling in any observed values from previous weeks in the season.

To obtain predictions for the peak incidence, we calculate the maximum simulated weekly case counts in each simulation trial.  We take our point estimate to be the mean of these values, and our distributional estimate is computed as the proportion of the simulation trials where the maximum fell in each bin.  Our predictions for the peak week and seasonal incidence are similar, but are based on different summaries of the simulated disease trajectories: the week with the largest simulated case counts in each simulation and the total simulated case counts for the season, respectively.

In theory, the predictive distributions for $\by_{t^*}^{(P)}$ obtained through the SSR method imply non-zero probabilities for each bin in the discretized distribution estimates required for the competition.  However, because we obtain our distributional estimates via simulation with a finite number of simulations, it sometimes occurs that the simulations result in a distributional estimate of 0 for one or more bins.  In these cases, we manually inflate the estimated probability of those bins by a small amount in order to preclude the possibility of achieving a logarithmic score of $- \infty$.

\section{Variables Used}
\label{sec:Variables}

There are several details specific to our application of the SSR method to the Dengue challenge data which we now discuss.  First, rather than directly using the observed weekly case counts in forming the weights, we transform them to a log scale and smooth them using the LOESS method as implemented by \texttt{R}'s \texttt{loess} function.  Due to time constraints, we have fixed the smoothing parameter at $12 / T$.  Ideally, we would estimate the smoothing parameter as a part of the model fitting process; we plan to explore this further in future work.

For the purpose of this competition, for prediction in a given location we have used the smoothed log-scale counts from that location, lag-1 smoothed log-scale counts from that location and the time index in calculating the observation weights.  We fit a separate component model for each possible prediction horizon $P \in \{4, 8, \ldots, 52\}$, corresponding to the number of weeks remaining in the season at the times when predictions are made.  The variables selected for inclusion in each of these component models are summarized in Table~\ref{tbl:VariablesUsedSanJuan} and Table~\ref{tbl:VariablesUsedIquitos}

\begin{table}[!p]
\centering
\begin{tabular}{>{\raggedleft}p{.05\textwidth} >{\centering\arraybackslash}p{.2\textwidth} >{\centering\arraybackslash}p{.2\textwidth} >{\centering\arraybackslash}p{.2\textwidth}}
\toprule
$P$ & Lag-1 Smoothed Log-Scale Weekly Counts & Smoothed Log-Scale Weekly Counts & Time Index \tabularnewline
\midrule
<<MakeVariablesUsedSanJuanTable, results="asis">>=
location <- "sanjuan"
location_for_ssr_fit_file <- "San_Juan"
read_env <- new.env()

for(P in seq(from=4, to=52, by=4)) {
    outputstr <- paste0(P, " & ")
    
    file_name <- paste0(
        "F:/Reich/dengue-ssr-prediction/competition-fits/fit-competition-ssr-ph",
        P,
        "-",
        location_for_ssr_fit_file,
        ".Rdata")
    
    load(file_name, envir=read_env)
    
    ssr_fit <- read_env$ssr_fit
    
    if("smooth_log_cases" %in% names(ssr_fit$lags_hat) &&
        1 %in% ssr_fit$lags_hat$smooth_log_cases) {
        outputstr <- paste0(outputstr, "X & ")
    } else {
        outputstr <- paste0(outputstr, " & ")
    }

    if("smooth_log_cases" %in% names(ssr_fit$lags_hat) &&
        0 %in% ssr_fit$lags_hat$smooth_log_cases) {
        outputstr <- paste0(outputstr, "X & ")
    } else {
        outputstr <- paste0(outputstr, " & ")
    }

    if("time_ind" %in% names(ssr_fit$lags_hat)) {
        outputstr <- paste0(outputstr, "X \\tabularnewline\n")
    } else {
        outputstr <- paste0(outputstr, " \\tabularnewline\n")
    }
    
    cat(outputstr)
}
@
\bottomrule
\end{tabular}
\caption{Variables selected for use by the model for each prediction horizon in the San Juan location.  An ``X" indicates that the given variable/lag combination was selected for use in computing observation weights.}
\label{tbl:VariablesUsedSanJuan}
\end{table}

\begin{table}[!p]
\centering
\begin{tabular}{>{\raggedleft}p{.05\textwidth} >{\centering\arraybackslash}p{.2\textwidth} >{\centering\arraybackslash}p{.2\textwidth} >{\centering\arraybackslash}p{.2\textwidth}}
\toprule
$P$ & Lag-1 Smoothed Log-Scale Weekly Counts & Smoothed Log-Scale Weekly Counts & Time Index \tabularnewline
\midrule
<<MakeVariablesUsedIquitosTable, results="asis">>=
location <- "iquitos"
location_for_ssr_fit_file <- "Iquitos"
read_env <- new.env()

for(P in seq(from=4, to=52, by=4)) {
    outputstr <- paste0(P, " & ")
    
    file_name <- paste0(
        "F:/Reich/dengue-ssr-prediction/competition-fits/fit-competition-ssr-ph",
        P,
        "-",
        location_for_ssr_fit_file,
        ".Rdata")
    
    load(file_name, envir=read_env)
    
    ssr_fit <- read_env$ssr_fit
    
    if("smooth_log_cases" %in% names(ssr_fit$lags_hat) &&
        1 %in% ssr_fit$lags_hat$smooth_log_cases) {
        outputstr <- paste0(outputstr, "X & ")
    } else {
        outputstr <- paste0(outputstr, " & ")
    }

    if("smooth_log_cases" %in% names(ssr_fit$lags_hat) &&
        0 %in% ssr_fit$lags_hat$smooth_log_cases) {
        outputstr <- paste0(outputstr, "X & ")
    } else {
        outputstr <- paste0(outputstr, " & ")
    }

    
    if("time_ind" %in% names(ssr_fit$lags_hat)) {
        outputstr <- paste0(outputstr, "X \\tabularnewline\n")
    } else {
        outputstr <- paste0(outputstr, " \\tabularnewline\n")
    }
    
    cat(outputstr)
}
@
\bottomrule
\end{tabular}
\caption{Variables selected for use by the model for each prediction horizon in the Iquitos location.  An ``X" indicates that the given variable/lag combination was selected for use in computing observation weights.}
\label{tbl:VariablesUsedIquitos}
\end{table}


\section{Computational Resources}
\label{sec:ComputationalResources}

We implemented our method using version 3.2.1 of the \texttt{R} statistical programming language \citep{RCore}.

\newpage

\section{Publications}
\label{sec:Publications}

\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{plainnat}
\bibliography{SSRbib}
\endgroup

\end{document}
