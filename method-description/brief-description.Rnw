\documentclass[fleqn]{article}\usepackage[]{graphicx}\usepackage[]{color}

\usepackage{geometry} 
\geometry{letterpaper, top=1.5cm, left=2cm, right=2cm}                

\usepackage[affil-it]{authblk}
\usepackage{amssymb, amsmath, amsthm, amsfonts}
%\usepackage{booktabs}
%\usepackage{array}
%\usepackage{paralist}
\usepackage{natbib}
\usepackage{enumerate}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}

\include{GrandMacros}
\newcommand{\norm}[1]{\vert\vert #1 \vert\vert}
\newcommand{\ind}{\mathbb{I}}

\usepackage{setspace}
\onehalfspacing


\title{Methods Description for Dengue Prediction \\ Reich Lab Team}
\date{}
\author{Evan L. Ray\thanks{Email: \texttt{elray@umass.edu}; Corresponding author}, Krzysztof Sakrejda, Stephen A. Lauer, Nicholas G. Reich}
\affil{Department of Biostatistics and Epidemiology, \\ University of Massachusetts -- Amherst, Amherst, MA}

\begin{document}

<<InitialBlock, echo = FALSE>>=
opts_knit$set(concordance=TRUE)
opts_chunk$set(echo=FALSE)

suppressMessages(library(lubridate))

suppressMessages(library(grid))
suppressMessages(library(ggplot2))

suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(reshape))

suppressMessages(library(ssr))
@

\maketitle

\section{Agreement}

By submitting these forecasts, we indicate our full and unconditional agreement to abide by the project's official rules and data use agreements.

\section{Introduction}
\label{sec:Intro}

In this document, we describe the method we use for predicting Dengue fever incidence in the 2015 Dengue Forecasting Project.  Briefly, we employ a non-parametric approach to prediction in dynamical systems referred to as state space reconstruction (SSR).  Our implementation of SSR is similar in spirit to the simplex projection method of \cite{sugihara1990nonlinearForecasting}; however, our approach is based on kernel regression and kernel density estimation (CITE CITE).  We describe the general SSR approach used to obtain predictive densities for counts in a given week in Section \ref{sec:PredStage1}.  We then discuss post-processing used to obtain predictions for the target quantities in the challenge.

\section{State Space Reconstruction}
\label{subsec:SSR:Motivation}

The spread of an infectious disease within a population is often represented as a dynamical process.  For example, the SIR model and its variants partition the population into disjoint groups of people according to their infection status, and describe changes in the size of each group over time with a system of differential equations.  These models make several assumptions regarding the number of states (i.e., disease categories) and the functional forms describing the rates at which people move through these categories over time.

A central challenge in the study of infectious disease is that we typically observe many fewer covariates than are important to the evolution of the process.  For example, even the most basic formulation of the SIR model involves the number of Susceptible and Infected individuals in the population at each time point.  Most infectious disease data sets include observations of the number of infected individuals in the population, but do not include measures of the number of susceptible individuals.  Furthermore, the SIR model is a very simplified representation of the evolution of the disease.  In reality, there are likely to be many other important factors affecting disease dynamics such as weather, immigration, and the presence of other diseases.  Many of these other factors may also be unobserved.

SSR is an alternate, non-parametric aproach to learning about dynamical systems that can be applied when we are uncertain of the functional forms describing evolution of the process and we do not have observations of all of the state variables.  The method can be motivated by Takens' theorem \citep{takens1981detectingattractors}, which establishes a connection between the original dynamical process and the process that is obtained by forming vectors of lagged observations.  Roughly, the theorem states that if the dynamical process satisfies certain conditions, there is a smooth, one-to-one and onto mapping with a smooth inverse between the limit sets of the original dynamical process and the limit sets of the lagged observation process. This provides some justification for studying the lagged observation process in order to learn about long-term dynamics of the system.  The theorem also tells us that we must include at least $2D + 1$ lags in the lagged observations in order for the limit sets to be equivalent, where $D$ is the number of states in the state space of the underlying dynamical process.

Although this theorem can help motivate the study of the lagged observation process, it does not answer two key questions for a practical implementation:
\begin{enumerate}
  \item If the dimension $D$ of the underlying process is unknown, we do not know how many or which lags should be included when we form the lagged observation process.
  \item It does not provide us with a specific method for studying the lagged observation process or using it to perform prediction.
\end{enumerate}

For this challenge, we use a non-parametric approach based on kernel regression and kernel density estimation to learn the relationship between the lagged observation process and its future values.  We use cross-validation to select the lags that are included and estimate the bandwidth parameters for the kernel functions.  Before formally stating the method in Subsection \ref{subsec:SSR:Statement}, we give an intuitive overview of it using the Dengue data from the challenge.  For the purposes of this demonstration of the method, we use a lag of one in forming the lagged observation vector.  We plot the weekly case counts in Figure~\ref{fig:SJPRWeeklyCaseCounts:a} and their transformation into a lagged coordinate space in Figure~\ref{fig:SJPRWeeklyCaseCounts:b}.

\begin{figure}[t!]
\centering
\begin{subfigure}[t]{0.475\textwidth}
<<SJPRWeeklyCaseCounts, fig.height=3.5, fig.width=3.5>>=
sj <- San_Juan_train

p <- ggplot(sj[sj$season %in% c("1990/1991", "1991/1992", "1992/1993"), ],
  aes(x=week_start_date, y=total_cases)) +
  geom_line() +
  xlab("Time") +
  ylab("Total Weekly Cases") +
  ggtitle("\nTotal Weekly Cases") +
  theme_bw()
print(p)
@
\caption{Total Weekly Cases over time}
\end{subfigure}
~
\begin{subfigure}[t]{0.475\textwidth}
<<SJPRWeeklyCaseCounts2, fig.height=3.5, fig.width=3.5>>=
sj <- sj %>% mutate(total_cases_lag1 = lag(total_cases))

p <- ggplot(sj[sj$season %in% c("1990/1991", "1991/1992", "1992/1993") & !is.na(sj$total_cases_lag1), ],
  aes(x=total_cases_lag1, y=total_cases, colour=season)) +
  geom_path() +
  xlab("Lagged Total Weekly Cases") +
  ylab("Total Weekly Cases") +
  ggtitle("Lagged Total Weekly Cases\nvs Total Weekly Cases") +
  theme_bw()
print(p)
@
\caption{Lagged Total Weekly Cases vs Total Weekly Cases}
\end{subfigure}
\caption{Total cases by week for San Juan, Puerto Rico.}
\label{fig:SJPRWeeklyCaseCounts}
\end{figure}

\section{Model Description}
\label{sec:ModelDescription}

Suppose we observe $\tilde{\bx}_t = \{\tilde{x}_{t,1}, \ldots, \tilde{x}_{t,D}\} \in \mathbb{R}^D$ at each point in time $t = 1, \ldots, T$.  Our goal is to obtain a predictive distribution for one of the observed variables over a range of prediction horizons spanning the remainder of the current season using lagged observations of all of the variables.  In order to do this, we employ weighted kernel density estimation. A more complete description of the methods used is available online \cite{Ray2015}.

We denote the index of the variable in $\tilde{\bx}_t$ that we are predicting by $d_{pred} \in \{1, \ldots, D\}$, and the prediction horizon by $P$.  Let $\bl^{max} = (l^{max}_1, \ldots, l^{max}_D)$ specify the maximum number of lags for each observed variable that may be used for prediction.  The lags that are actually used in prediction will be estimated by a procedure we describe in Section ***
It will be convenient to set
\begin{align*}
&\by_t^{(P)} = (F^{(1)} \tilde{x}_{t, d_{pred}}, \ldots, F^{(P)} \tilde{x}_{t, d_{pred}}) \text{ and} \\
&\bx_t^{(\bl^{max})} = (\tilde{x}_{1,t}, \ldots, B^{(l^{max}_1)} \tilde{x}_{1, t}, \ldots, \tilde{x}_{D,t}, \ldots, B^{(l^{max}_D)} \tilde{x}_{D, t})
\end{align*}
where, $F^{(a)}$ is the forward shift operator defined by $F^{(a)} \tilde{x}_{t, d} = \tilde{x}_{t + a, d}$ and $B^{(a)}$ is the backward shift operator defined by $B^{(a)} \tilde{x}_{t, d} = \tilde{x}_{t - a, d}$.  The variable $\by_t^{(P)}$ represents the prediction target when our most recent observation was made at time $t$: the vector of counts for the following $P$ weeks.  The variable $\bx_t^{(\bl^{max})}$ represents the vector of all lagged covariates that are available for use in performing prediction.  We wish to estimate the distribution of $\by_t^{(P)} | \bx_t^{(\bl^{max})}$.

The general method for fitting and predicting $\by_{t^*}^{(P)}$ at some particular time $t^*$ can be summarized as follows. We use a weighting kernel $K^{wt}$ to assign a weight to each time $t$ that reflects how similar the lagged observation vector formed at time $t$ is to the lagged observation vector formed at the time $t^*$.  If $\bx_t$ is similar to $\bx_{t^*}$, a large weight is assigned to $t$; if $\bx_t$ is different from $\bx_{t^*}$, a small weight is assigned to $t$.  These weights are then used to obtain a weighted kernel density estimate in Equation \eqref{eqn:PredLine1}, placing a kernel centered at $\by_t^{(P)}$ with the corresponding weight $w_{t, t^*}$.

We have employed two functional forms for the kernel functions in our work.  We use the squared exponential kernel for most of the component kernel functions in both $K^{wt}$ and $K^{pred}$.  When this kernel is used as a component in $K^{pred}$, we require that the kernel function integrates to 1 in order to obtain a proper density estimate.  

The second kernel we have employed in the weighting kernel $K^{wt}$ is the periodic kernel, which is commonly used in the Gaussian Processes literature.  We have employed the periodic kernel only for the component of the weighting kernel corresponding to the time index variable.  Effectively, this means that observations are scored as being similar to each other if they were recorded at a similar time of the year.

There are several details specific to our application of the SSR method to the Dengue challenge data which we now discuss.  First, it can be seen from the plots in Figure \ref{fig:SJPRWeeklyCaseCounts} that the weekly case counts are not smooth over time.  The plot in panel (a) of the figure shows that the weekly case counts are jagged, with many small peaks around a larger trend.  In panel (b), we see that this induces some "loops" in the lagged covariate space when a lag of one week is used.  These loops can cause difficulties for the SSR method, because lagged covariate vectors representing different points in the disease's evolution may be near each other.  This problem can be addressed in several ways.  One possibility is the use of a different lag; for example, these loops might be reduced if a lag of three weeks were used instead of one week.

Another alternative that we pursue is the use of smoothed observations in forming the lagged observation vectors.  This is appropriate because we are most interested in capturing long-term trends.  We therefore perform an initial smooth using the LOESS method.  We use these smoothed case counts on a log scale for the weighting kernels, and the unsmoothed case counts on the original scale for the prediction kernels.

\section{Parameter Estimation}
\label{sec:Estimation}

We use cross-validation in order to select the variables that are used in the model and estimate the corresponding bandwidth parameters by (approximately) minimizing a cross-validation estimate of the mean absolute error (MAE) of the point predictions obtained from the model.  

We use a forward/backward stagewise procedure to obtain the set of combinations of variables and lags that are included in the final model (represented by $\bu$).  For each candidate model, we use the limited memory box constrained optimization procedure of \cite{byrd1995limitedmemoryoptim} to estimate the bandwidth parameters.

%In order to obtain the point estimate $\widehat{y}_{t^*}^{(p)}$ for each time point $t^*$ and number of steps ahead $p$, we first form the predictive density as in Equations \eqref{eqn:PredLine1} through \eqref{eqn:PredLine4}.  In order to reduce the potential for our parameter estimates to be affected by local correlation in the time series, we eliminate all time points that fall within one year of $t^*$ from the index set $\tau$; we refer to this timepoint-specific index set as $\tau^*$.  We then take our point estimate to be the expected value of this estimated predictive distribution. We note that for symmetric prediction kernels, this point estimate does not depend on the bandwidth parameters.

For the prediction kernels, we estimated the bandwidth parameters using the methods of \cite{sheather1991KDEbw}, as implemented in \texttt{R}'s \texttt{bw.SJ} function.  We estimate the bandwidth parameter for each prediction horizon $p \in \{1, \ldots, P\}$ separately.

\section{Predictions for Challenge}
\label{sec:PredictionsForChallenge}



\section{Variables}
\label{sec:Variables}



\section{Computational Resources}
\label{sec:ComputationalResources}

We implemented our variation on SSR in the \texttt{R} statistical programming language \citep{RCore}.

\section{Publications}
\label{sec:Publications}

\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{plainnat}
\bibliography{SSRbib}
\endgroup

\end{document}
